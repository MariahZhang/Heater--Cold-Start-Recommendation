{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('load entity name set')\n",
    "name_set = set()\n",
    "with open('FB2M_entity.txt', 'r') as FB_name_file:\n",
    "    for line in FB_name_file:\n",
    "        name_set.add(line.split('\\t')[2].replace('\\n', ''))\n",
    "print('#entity names = ' + str(len(name_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_dir = './2019/'\n",
    "\n",
    "device_sid2iid_dict = dict()\n",
    "entity_name2iid_dict = dict()\n",
    "data_list = []\n",
    "\n",
    "filter_set = {'youtube', 'movies', 'netflix', 'channel', 'show', 'null', 'find', 'watch', 'silence', 'hbo', \n",
    "              'play', 'recordings', 'tv', 'record', 'search', 'free', 'you', 'show me', 'exit', 'recording', \n",
    "              'yes', 'please', 'amazon', 'hulu', 'turn off', 'weather', 'network', 'the', 'pause'}\n",
    "\n",
    "for month_dir in os.listdir(year_dir):\n",
    "    if month_dir in ['02', '03']:\n",
    "        continue\n",
    "    for day_dir in os.listdir(year_dir + month_dir):\n",
    "        print(year_dir + month_dir + '/' + day_dir, end=' :: ')\n",
    "        for file in os.listdir(year_dir + month_dir + '/' + day_dir):\n",
    "            filename = file\n",
    "            if filename.endswith(\".json\"): \n",
    "                file_path = year_dir + month_dir + '/' + day_dir + '/' + filename\n",
    "                with open(file_path) as fp:  \n",
    "                    for line in fp:\n",
    "                        line_json = json.loads(line)\n",
    "                        device_sid = line_json['deviceId']\n",
    "                        utterances = line_json['utterances']\n",
    "                        for utterance in utterances:\n",
    "                            timestamp = int(utterance[0])\n",
    "                            query = utterance[1].replace('\\n', '')\n",
    "                            if query in name_set and query not in filter_set and not query.isdigit():\n",
    "                                if device_sid not in device_sid2iid_dict:\n",
    "                                    device_iid = len(device_sid2iid_dict)\n",
    "                                    device_sid2iid_dict[device_sid] = device_iid\n",
    "                                else:\n",
    "                                    device_iid = device_sid2iid_dict[device_sid]\n",
    "                                if query not in entity_name2iid_dict:\n",
    "                                    entity_iid = len(entity_name2iid_dict)\n",
    "                                    entity_name2iid_dict[query] = entity_iid\n",
    "                                else:\n",
    "                                    entity_iid = entity_name2iid_dict[query]\n",
    "                                data_list.append([device_iid, entity_iid, timestamp])\n",
    "        print(len(data_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(device_sid2iid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entity_name2iid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_iid2sid_list = [''] * len(device_sid2iid_dict)\n",
    "entity_iid2name_list = [''] * len(entity_name2iid_dict)\n",
    "for i, sid in enumerate(device_sid2iid_dict):\n",
    "    device_iid2sid_list[device_sid2iid_dict[sid]] = sid\n",
    "for i, name in enumerate(entity_name2iid_dict):\n",
    "    entity_iid2name_list[entity_name2iid_dict[name]] = name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = np.array(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data_array.npy', data_list)\n",
    "np.save('device_iid2sid_list.npy', np.array(device_iid2sid_list))\n",
    "np.save('entity_iid2name_list.npy', np.array(entity_iid2name_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'device':data_list[:, 0], 'entity':data_list[:, 1], 'timestamp':data_list[:, 2]})\n",
    "df.drop_duplicates(subset=['device', 'entity'], keep='last', inplace=True) \n",
    "data_list = None\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"14/06/2019\"\n",
    "time_split = time.mktime(datetime.datetime.strptime(s, \"%d/%m/%Y\").timetuple()) * 1000\n",
    "time_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['freq'] = df.groupby('entity')['entity'].transform('count')\n",
    "df.drop(df.index[df['freq'] > 5000 ], inplace=True)\n",
    "df.drop(['freq'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['freq'] = df.groupby('device')['device'].transform('count')\n",
    "df.drop(df.index[df['freq'] < 20], inplace=True)\n",
    "df['freq'] = df.groupby('entity')['entity'].transform('count')\n",
    "df.drop(df.index[df['freq'] < 20], inplace=True)\n",
    "df['freq'] = df.groupby('device')['device'].transform('count')\n",
    "print(df['freq'].min())\n",
    "\n",
    "df.drop(['freq'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['freq'] = df.groupby('device')['device'].transform('count')\n",
    "df.drop(df.index[df['freq'] < 20], inplace=True)\n",
    "df['freq'] = df.groupby('entity')['entity'].transform('count')\n",
    "df.drop(df.index[df['freq'] < 20], inplace=True)\n",
    "df['freq'] = df.groupby('device')['device'].transform('count')\n",
    "print(df['freq'].min())\n",
    "\n",
    "df.drop(['freq'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['freq'] = df.groupby('device')['device'].transform('count')\n",
    "df.drop(df.index[df['freq'] < 20], inplace=True)\n",
    "df['freq'] = df.groupby('entity')['entity'].transform('count')\n",
    "df.drop(df.index[df['freq'] < 20], inplace=True)\n",
    "df['freq'] = df.groupby('device')['device'].transform('count')\n",
    "print(df['freq'].min())\n",
    "\n",
    "df.drop(['freq'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['freq'] = df.groupby('device')['device'].transform('count')\n",
    "df.drop(df.index[df['freq'] < 20], inplace=True)\n",
    "df['freq'] = df.groupby('entity')['entity'].transform('count')\n",
    "df.drop(df.index[df['freq'] < 20], inplace=True)\n",
    "df['freq'] = df.groupby('device')['device'].transform('count')\n",
    "print(df['freq'].min())\n",
    "\n",
    "df.drop(['freq'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of device = ' + str(len(df['device'].unique())))\n",
    "print('number of entity = ' + str(len(df['entity'].unique())))\n",
    "print('number of interaction = ' + str(len(df)))\n",
    "print('sparsity = ' + str(len(df) * 1.0 / (len(df['device'].unique()) * len(df['entity'].unique()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"First, split training and test sets by time\"\"\"\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_df = copy.copy(df)\n",
    "test_df = copy.copy(df)\n",
    "\n",
    "train_df.drop(train_df.index[train_df['timestamp'] >= time_split], inplace=True)\n",
    "test_df.drop(test_df.index[test_df['timestamp'] < time_split], inplace=True)\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# devices in training set = ' + str(len(train_df['device'].unique())))\n",
    "print('# entities in training set = ' + str(len(train_df['entity'].unique())))\n",
    "print('# interactions in training set = ' + str(len(train_df)))\n",
    "print('sparsity in training set = ' + str(len(train_df) * 1.0 / (len(train_df['device'].unique()) * len(train_df['entity'].unique()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# devices in test set = ' + str(len(test_df['device'].unique())))\n",
    "print('# entities in test set = ' + str(len(test_df['entity'].unique())))\n",
    "print('# interactions in test set = ' + str(len(test_df)))\n",
    "print('sparsity in test set = ' + str(len(test_df) * 1.0 / (len(test_df['device'].unique()) * len(test_df['entity'].unique()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"randomly pick cold start entities in test set\"\"\"\n",
    "\n",
    "test_entity_array = test_df['entity'].unique()\n",
    "entity_in_test_notin_train_set = set(test_df['entity'].unique()) - set(train_df['entity'].unique())\n",
    "cold_start_entity_array = np.array(list(set(np.random.choice(test_entity_array, \n",
    "                                                             size=int(0.3 * len(test_entity_array)))).union(entity_in_test_notin_train_set)))\n",
    "print('Cold Start Entity set size = ' + str(len(cold_start_entity_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entity_in_cold_start_set = set(train_df['entity'].unique()).intersection(set(cold_start_entity_array))\n",
    "print('# entities of training set belong to cold start = ' + str(len(train_entity_in_cold_start_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"remove all the interactions with the cold start entities in training set\"\"\"\n",
    "\n",
    "train_df = train_df[~train_df['entity'].isin(cold_start_entity_array)]\n",
    "print('# devices in training set = ' + str(len(train_df['device'].unique())))\n",
    "print('# entities in training set = ' + str(len(train_df['entity'].unique())))\n",
    "print('# interactions in training set = ' + str(len(train_df)))\n",
    "print('sparsity in training set = ' + str(len(train_df) * 1.0 / (len(train_df['device'].unique()) * len(train_df['entity'].unique()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_device_array = train_df['device'].unique()\n",
    "test_df = test_df[test_df['device'].isin(train_device_array)]\n",
    "print('# devices in test set = ' + str(len(test_df['device'].unique())))\n",
    "print('# entities in test set = ' + str(len(test_df['entity'].unique())))\n",
    "print('# cold start entities in test set = ' + str(len(set(test_df['entity'].unique()).intersection(set(cold_start_entity_array)))))\n",
    "print('# training entities in test set = ' + str(len(set(test_df['entity'].unique()).intersection(set(train_df['entity'].unique())))))\n",
    "print('# interactions in test set = ' + str(len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "new_df = pd.concat([train_df, test_df])\n",
    "new_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"relabel the device integer ids\"\"\"\n",
    "\n",
    "device_dict = dict()\n",
    "device_list = new_df['device'].values\n",
    "for d in device_list:\n",
    "    if d not in device_dict:\n",
    "        tmp = len(device_dict)\n",
    "        device_dict[d] = tmp\n",
    "\n",
    "\n",
    "\"\"\"relabel the entity integer ids\"\"\"\n",
    "\n",
    "entity_dict = dict()\n",
    "entity_list = new_df['entity'].values\n",
    "# entity_list = np.random.permutation(entity_list)\n",
    "entity_id2name_list = []\n",
    "entity_name2id_dict = dict()\n",
    "for e in entity_list:\n",
    "    if e not in entity_dict:\n",
    "        tmp = len(entity_dict)\n",
    "        entity_dict[e] = tmp\n",
    "        entity_id2name_list.append(entity_iid2name_list[e])\n",
    "        entity_name2id_dict[entity_iid2name_list[e]] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = train_df.values\n",
    "new_device_list = []\n",
    "new_entity_list = []\n",
    "for i in range(train_array.shape[0]):\n",
    "    new_device_list.append(device_dict[train_array[i, 0]])\n",
    "    new_entity_list.append(entity_dict[train_array[i, 1]])\n",
    "train_df['device'] = new_device_list\n",
    "train_df['entity'] = new_entity_list\n",
    "\n",
    "test_array = test_df.values\n",
    "new_device_list = []\n",
    "new_entity_list = []\n",
    "for i in range(test_array.shape[0]):\n",
    "    new_device_list.append(device_dict[test_array[i, 0]])\n",
    "    new_entity_list.append(entity_dict[test_array[i, 1]])\n",
    "test_df['device'] = new_device_list\n",
    "test_df['entity'] = new_entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_item = len(entity_id2name_list)\n",
    "item_reset_index = np.random.permutation(num_item)\n",
    "train_df['entity'] = item_reset_index[train_df['entity'].values]\n",
    "test_df['entity'] = item_reset_index[test_df['entity'].values]\n",
    "\n",
    "new_entity_id2name_list = copy.copy(entity_id2name_list)\n",
    "for i in range(num_item):\n",
    "    name = entity_id2name_list[i]\n",
    "    new_entity_id2name_list[item_reset_index[i]] = name\n",
    "\n",
    "for name in entity_name2id_dict:\n",
    "    old_index = entity_name2id_dict[name]\n",
    "    entity_name2id_dict[name] = item_reset_index[old_index]\n",
    "entity_id2name_list = new_entity_id2name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_warm = train_df['entity'].unique()\n",
    "item_cold = np.array(list(set(test_df['entity'].unique()) - set(item_warm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = (np.array([list(train_df['entity'].value_counts().index), list(train_df['entity'].value_counts().values)])).T\n",
    "mat = sorted(mat, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-1, -11, -1):\n",
    "    print(str(mat[i][1]) + ' :: ' + str(entity_id2name_list[mat[i][0]]))\n",
    "print('')\n",
    "    \n",
    "for i in range(20):\n",
    "    print(str(mat[i][1]) + ' :: ' + str(entity_id2name_list[mat[i][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# devices = ' + str(len(new_df['device'].unique())))\n",
    "print('# entities = ' + str(len(new_df['entity'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./entity_id2name_list.pkl', 'wb') as f:\n",
    "    pickle.dump(entity_id2name_list, f)\n",
    "with open('./entity_name2id_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(entity_name2id_dict, f)\n",
    "train_df.to_csv('./train_df.csv', index=False)\n",
    "test_df.to_csv('./test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_entity_set = set(test_df['entity'].unique()) - set(train_df['entity'].unique())\n",
    "warm_entity_set = set(train_df['entity'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(warm_entity_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cold_entity_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_item = len(entity_dict)\n",
    "num_user = len(device_dict)\n",
    "# item_like = []  # for each item, list of users who like it\n",
    "# item_unlike = []  # for each item, list of users who do not like it\n",
    "user_like = []  # for each user, list of items the user likes\n",
    "user_like_test_cs = []  # for each user in test set, the list of items the user likes\n",
    "user_like_test_ncs = []  # for each user in test set, the list of items the user likes\n",
    "\n",
    "train_array = train_df[['device', 'entity']].values\n",
    "test_array = test_df[['device', 'entity']].values\n",
    "user_set = set(range(num_user))\n",
    "\n",
    "for i in range(num_user):\n",
    "    like_item = (train_array[list(np.where(train_array[:, 0] == i)[0]), 1]).tolist()\n",
    "    test_like_item_set = set((test_array[list(np.where(test_array[:, 0] == i)[0]), 1]).tolist())\n",
    "    user_like.append(like_item)\n",
    "    user_like_test_cs.append(list(test_like_item_set.intersection(cs_entity_set)))\n",
    "    user_like_test_ncs.append(list(test_like_item_set.intersection(ncs_entity_set)))\n",
    "    \n",
    "# np.save('./item_like.npy', np.array(item_like))\n",
    "np.save('./user_like_all.npy', np.array(user_like))\n",
    "np.save('./user_like_test_cold.npy', np.array(user_like_test_cs))\n",
    "np.save('./user_like_test_warm.npy', np.array(user_like_test_ncs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./info.pkl', 'wb') as f:\n",
    "    pickle.dump({'num_user': num_user, 'num_item': num_item}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entity_set = set(train_df['entity'].unique())\n",
    "test_entity_set = set(test_df['entity'].unique())\n",
    "\n",
    "cold_entity_set = test_entity_set - train_entity_set\n",
    "cold_entity_array = np.array(list(cold_entity_set))\n",
    "train_entity_array = np.array(list(train_entity_set))\n",
    "\n",
    "test_cold_df = test_df[test_df['entity'].isin(cold_entity_array)]\n",
    "test_warm_df = test_df[test_df['entity'].isin(train_entity_array)]\n",
    "\n",
    "test_cold_df.to_csv('./test_cold_df.csv', index=False)\n",
    "test_warm_df.to_csv('./test_warm_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
